{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In the feature engineering step, we create new features or modify existing ones to improve the performance of the machine learning model.\n",
    "\n",
    "Helpful Links:\n",
    "https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables referencing modules in repository\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics as stat \n",
    "import pipeline as pipe\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from src.features import build_features\n",
    "# from src.data import make_dataset \n",
    "# commented out because: there seems to be an issue at the moment with the initial method from make_dataset\n",
    "from src.models import train_model\n",
    "from src.models import predict_model\n",
    "from src.visualization import visualize\n",
    "from tabulate import tabulate\n",
    "from scipy import stats\n",
    "from src.pipelines.build_pipelines import CustomPipeline, get_best_steps\n",
    "from sklearn import set_config\n",
    "\n",
    "from src.features.build_features import *\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline below includes the current best steps we found out to create a model with the highest evaluation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "evaluating pipeline\n",
      "    fit_time: 0.23234968185424804\n",
      "    score_time: 0.028180503845214845\n",
      "    test_accuracy: 0.70739972086257\n",
      "    test_f1-score: 0.6118018211054055\n",
      "    test_mcc: 0.4365692823656165\n",
      "storing model and prediction\n"
     ]
    }
   ],
   "source": [
    "# Confirm the pipeline is working in the notebook\n",
    "testpip = CustomPipeline(get_best_steps(), apply_ordinal_encoding=True, force_data_cleaning=False)\n",
    "testpip.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;feature_remover&#x27;,\n",
       "                 RemoveFeatureTransformer(features_to_drop=[])),\n",
       "                (&#x27;feature_engineering&#x27;, DummyTransformer()),\n",
       "                (&#x27;discretizer&#x27;,\n",
       "                 CustomColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                         transformers=[(&#x27;bins&#x27;,\n",
       "                                                        KBinsDiscretizer(encode=&#x27;ordinal&#x27;,\n",
       "                                                                         n_bins=2,\n",
       "                                                                         strategy=&#x27;uniform&#x27;),\n",
       "                                                        &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C497...\n",
       "                (&#x27;encoder_and_scaler&#x27;,\n",
       "                 CustomColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                         transformers=[(&#x27;encoder&#x27;,\n",
       "                                                        BinaryEncoder(),\n",
       "                                                        &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C49734DB90&gt;),\n",
       "                                                       (&#x27;scaler&#x27;,\n",
       "                                                        RobustScaler(),\n",
       "                                                        &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C497495810&gt;)])),\n",
       "                (&#x27;estimator&#x27;, DecisionTreeClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;feature_remover&#x27;,\n",
       "                 RemoveFeatureTransformer(features_to_drop=[])),\n",
       "                (&#x27;feature_engineering&#x27;, DummyTransformer()),\n",
       "                (&#x27;discretizer&#x27;,\n",
       "                 CustomColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                         transformers=[(&#x27;bins&#x27;,\n",
       "                                                        KBinsDiscretizer(encode=&#x27;ordinal&#x27;,\n",
       "                                                                         n_bins=2,\n",
       "                                                                         strategy=&#x27;uniform&#x27;),\n",
       "                                                        &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C497...\n",
       "                (&#x27;encoder_and_scaler&#x27;,\n",
       "                 CustomColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                         transformers=[(&#x27;encoder&#x27;,\n",
       "                                                        BinaryEncoder(),\n",
       "                                                        &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C49734DB90&gt;),\n",
       "                                                       (&#x27;scaler&#x27;,\n",
       "                                                        RobustScaler(),\n",
       "                                                        &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C497495810&gt;)])),\n",
       "                (&#x27;estimator&#x27;, DecisionTreeClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveFeatureTransformer</label><div class=\"sk-toggleable__content\"><pre>RemoveFeatureTransformer(features_to_drop=[])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyTransformer</label><div class=\"sk-toggleable__content\"><pre>DummyTransformer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">discretizer: CustomColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>CustomColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                        transformers=[(&#x27;bins&#x27;,\n",
       "                                       KBinsDiscretizer(encode=&#x27;ordinal&#x27;,\n",
       "                                                        n_bins=2,\n",
       "                                                        strategy=&#x27;uniform&#x27;),\n",
       "                                       &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C4977845D0&gt;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">bins</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C4977845D0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KBinsDiscretizer</label><div class=\"sk-toggleable__content\"><pre>KBinsDiscretizer(encode=&#x27;ordinal&#x27;, n_bins=2, strategy=&#x27;uniform&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;geo_level_1_id&#x27;, &#x27;geo_level_2_id&#x27;, &#x27;geo_level_3_id&#x27;, &#x27;land_surface_condition&#x27;, &#x27;foundation_type&#x27;, &#x27;roof_type&#x27;, &#x27;ground_floor_type&#x27;, &#x27;other_floor_type&#x27;, &#x27;position&#x27;, &#x27;has_superstructure_adobe_mud&#x27;, &#x27;has_superstructure_mud_mortar_stone&#x27;, &#x27;has_superstructure_cement_mortar_stone&#x27;, &#x27;has_superstructure_cement_mortar_brick&#x27;, &#x27;has_superstructure_timber&#x27;, &#x27;has_superstructure_bamboo&#x27;, &#x27;has_secondary_use_agriculture&#x27;, &#x27;has_secondary_use_hotel&#x27;, &#x27;has_secondary_use_rental&#x27;, &#x27;has_secondary_use_institution&#x27;, &#x27;has_secondary_use_school&#x27;, &#x27;has_secondary_use_industry&#x27;, &#x27;has_secondary_use_health_post&#x27;, &#x27;has_secondary_use_gov_office&#x27;, &#x27;has_secondary_use_use_police&#x27;, &#x27;has_secondary_use_other&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">encoder_and_scaler: CustomColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>CustomColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                        transformers=[(&#x27;encoder&#x27;, BinaryEncoder(),\n",
       "                                       &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C49734DB90&gt;),\n",
       "                                      (&#x27;scaler&#x27;, RobustScaler(),\n",
       "                                       &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C497495810&gt;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">encoder</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C49734DB90&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BinaryEncoder</label><div class=\"sk-toggleable__content\"><pre>BinaryEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">scaler</label><div class=\"sk-toggleable__content\"><pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001C497495810&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('feature_remover',\n",
       "                 RemoveFeatureTransformer(features_to_drop=[])),\n",
       "                ('feature_engineering', DummyTransformer()),\n",
       "                ('discretizer',\n",
       "                 CustomColumnTransformer(remainder='passthrough',\n",
       "                                         transformers=[('bins',\n",
       "                                                        KBinsDiscretizer(encode='ordinal',\n",
       "                                                                         n_bins=2,\n",
       "                                                                         strategy='uniform'),\n",
       "                                                        <sklearn.compose._column_transformer.make_column_selector object at 0x000001C497...\n",
       "                ('encoder_and_scaler',\n",
       "                 CustomColumnTransformer(remainder='passthrough',\n",
       "                                         transformers=[('encoder',\n",
       "                                                        BinaryEncoder(),\n",
       "                                                        <sklearn.compose._column_transformer.make_column_selector object at 0x000001C49734DB90>),\n",
       "                                                       ('scaler',\n",
       "                                                        RobustScaler(),\n",
       "                                                        <sklearn.compose._column_transformer.make_column_selector object at 0x000001C497495810>)])),\n",
       "                ('estimator', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_config(display=\"diagram\")\n",
    "testpip.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "evaluating pipeline\n",
      "    fit_time: 0.18929929733276368\n",
      "    score_time: 0.016856908798217773\n",
      "    test_accuracy: 0.6685463596585042\n",
      "    test_f1-score: 0.564354039222139\n",
      "    test_mcc: 0.3647947219811763\n",
      "storing model and prediction\n"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeClassifier()\n",
    "baseline_pipe = CustomPipeline(steps=[('estimator', estimator)], skip_evaluation=False)\n",
    "baseline_pipe.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above will be used as a baseline model for a decision tree classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic - Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding (Patrick)\n",
    "\n",
    "Encoding is a process used to transform categorical data into numerical values that can be understood by machine learning algorithms. There are several types of encoding techniques used in feature engineering such as one-hot encoding and label encoding. Some of the most commonly used techniques are:\n",
    "\n",
    "* One-hot encoding\n",
    "* Label encoding\n",
    "* Binary encoding\n",
    "* Count encoding\n",
    "* Target encoding (Thomas)\n",
    "* Hashing encoding\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical data into numerical data by creating a binary vector for each category. For example, if we have a categorical feature called “color” with three categories (red, green, and blue), we can create three binary vectors (one for each category) with a value of 1 for the corresponding category and 0 for the others.\n",
    "\n",
    "Label encoding is another technique used to convert categorical data into numerical data by assigning a unique integer value to each category. For example, if we have a categorical feature called “color” with three categories (red, green, and blue), we can assign the values 0, 1, and 2 to each category respectively.\n",
    "\n",
    "Binary encoding is similar to one-hot encoding but uses fewer features. Count encoding replaces each category with the number of times it appears in the dataset. Target encoding replaces each category with the mean target value for that category. Hashing encoding is a technique that maps each category to a fixed-length vector\n",
    "\n",
    "Source: https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization (Marco)\n",
    "\n",
    "Discretization is a process used to transform continuous data into categorical data. It involves dividing the range of a continuous variable into a set of intervals or bins and then assigning each value to the corresponding bin.\n",
    "\n",
    "Binning or discretization is used for the transformation of a continuous or numerical variable into a categorical feature. Binning of continuous variable introduces non-linearity and tends to improve the performance of the model. It can also be used to identify missing values or outliers\n",
    "\n",
    "Discretization can help improve the classifier by reducing the noise in the data and making it easier for the classifier to identify patterns. By discretizing continuous variables, they may be transformed into categorical variables that are easier to work with. This can help improve the accuracy of the classifier by reducing the number of features and making it easier to identify which features are most important.\n",
    "\n",
    "The effectiveness of discretization can depend on the model applied. Some models may be more sensitive to the choice of discretization method than others. Many machine learning algorithms perform better when tey are trained with discrete variables. For example, decision trees and random forests can benefit from discretization because they work best with categorical variables. On the other hand, linear regression models may not benefit as much from discretization because they work best with continuous variables\n",
    "\n",
    "Source: https://towardsdatascience.com/an-intro-to-discretization-techniques-for-machine-learning-93dce1198e68"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take a look at equal frequency discretization. This entails transforming continuous data into bins, with each bin having the same (or similar) number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "The best score was achieved with bins of 2:\n",
      "    fit_time: 0.17308621406555175\n",
      "    score_time: 0.021747493743896486\n",
      "    test_accuracy: 0.6976863404460938\n",
      "    test_f1-score: 0.6012375853468177\n",
      "    test_mcc: 0.41910157810277127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "    \n",
    "# trains and predicts on the transformed data\n",
    "estimator = DecisionTreeClassifier()\n",
    "bestScore = {}\n",
    "bestBins = 0\n",
    "for i in range(2,10):    \n",
    "    kbins = KBinsDiscretizer(n_bins=i, strategy='quantile', encode='ordinal')\n",
    "    testpip = CustomPipeline([     \n",
    "        ('discretizer', CustomColumnTransformer([\n",
    "            ('bins', kbins, make_column_selector(dtype_include=['float64']))\n",
    "        ], remainder='passthrough')),\n",
    "        ('estimator', estimator)],\n",
    "        skip_evaluation=True,\n",
    "        skip_storing=True)\n",
    "    testpip.run()\n",
    "    testpip.evaluate(testpip.pipeline, testpip.X_train, testpip.y_train, False)\n",
    "    \n",
    "    if len(bestScore) <= 0:\n",
    "        bestScore = testpip.evaluation_scoring\n",
    "        bestBins = i\n",
    "        \n",
    "    elif bestScore['test_mcc'].mean() < testpip.evaluation_scoring['test_mcc'].mean():\n",
    "        bestScore = testpip.evaluation_scoring\n",
    "        bestBins = i\n",
    "\n",
    "print(f'The best score was achieved with bins of {bestBins}:')\n",
    "for score in bestScore:\n",
    "    print('    ' + score + ':', bestScore[score].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the baseline: </br>\n",
    "The test_accuracy improved from 0.665 to 0.697.</br>\n",
    "The f1_score improved from 0.562 to 0.601.</br>\n",
    "The mcc score improved from 0.359 to 0.418"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take a look at equal-width discretization. As the name suggests it transforms data into bins with the same width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "The best score was achieved with bins of 2:\n",
      "    fit_time: 0.16273894309997558\n",
      "    score_time: 0.021566247940063475\n",
      "    test_accuracy: 0.7075452266655814\n",
      "    test_f1-score: 0.611777011823585\n",
      "    test_mcc: 0.4368278155365372\n"
     ]
    }
   ],
   "source": [
    "# trains and predicts on the transformed data\n",
    "estimator = DecisionTreeClassifier()\n",
    "bestScore = {}\n",
    "bestBins = 0\n",
    "for i in range(2,10):    \n",
    "    kbins = KBinsDiscretizer(n_bins=i, strategy='uniform', encode='ordinal')\n",
    "    testpip = CustomPipeline([     \n",
    "        ('discretizer', CustomColumnTransformer([\n",
    "            ('bins', kbins, make_column_selector(dtype_include=['float64']))\n",
    "        ], remainder='passthrough')),\n",
    "        ('estimator', estimator)],\n",
    "        skip_evaluation=True,\n",
    "        skip_storing=True)\n",
    "    testpip.run()\n",
    "    testpip.evaluate(testpip.pipeline, testpip.X_train, testpip.y_train, False)\n",
    "    \n",
    "    if len(bestScore) <= 0:\n",
    "        bestScore = testpip.evaluation_scoring\n",
    "        bestBins = i\n",
    "        \n",
    "    elif bestScore['test_mcc'].mean() < testpip.evaluation_scoring['test_mcc'].mean():\n",
    "        bestScore = testpip.evaluation_scoring\n",
    "        bestBins = i\n",
    "\n",
    "print(f'The best score was achieved with bins of {bestBins}:')\n",
    "for score in bestScore:\n",
    "    print('    ' + score + ':', bestScore[score].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the baseline: </br>\n",
    "The test_accuracy improved from 0.665 to 0.707.</br>\n",
    "The f1_score improved from 0.562 to 0.611.</br>\n",
    "The mcc score improved from 0.359 to 0.437"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we take a look at k-means discretization. The k-means discretization entails using the k-means clustering algorithm to assign data points to bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "The best score was achieved with bins of 2:\n",
      "    fit_time: 0.23662548065185546\n",
      "    score_time: 0.026444244384765624\n",
      "    test_accuracy: 0.6997054714674864\n",
      "    test_f1-score: 0.607576314494165\n",
      "    test_mcc: 0.4224944366934965\n"
     ]
    }
   ],
   "source": [
    "# trains and predicts on the transformed data\n",
    "estimator = DecisionTreeClassifier()\n",
    "bestScore = {}\n",
    "bestBins = 0\n",
    "for i in range(2,10):    \n",
    "    kbins = KBinsDiscretizer(n_bins=i, strategy='kmeans', encode='ordinal')\n",
    "    testpip = CustomPipeline([     \n",
    "        ('discretizer', CustomColumnTransformer([\n",
    "            ('bins', kbins, make_column_selector(dtype_include=['float64']))\n",
    "        ], remainder='passthrough')),\n",
    "        ('estimator', estimator)],\n",
    "        skip_evaluation=True,\n",
    "        skip_storing=True)\n",
    "    testpip.run()\n",
    "    testpip.evaluate(testpip.pipeline, testpip.X_train, testpip.y_train, False)\n",
    "    \n",
    "    if len(bestScore) <= 0:\n",
    "        bestScore = testpip.evaluation_scoring\n",
    "        bestBins = i\n",
    "        \n",
    "    elif bestScore['test_mcc'].mean() < testpip.evaluation_scoring['test_mcc'].mean():\n",
    "        bestScore = testpip.evaluation_scoring\n",
    "        bestBins = i\n",
    "\n",
    "print(f'The best score was achieved with bins of {bestBins}:')\n",
    "for score in bestScore:\n",
    "    print('    ' + score + ':', bestScore[score].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the baseline: </br>\n",
    "The test_accuracy improved from 0.665 to 0.699.</br>\n",
    "The f1_score improved from 0.562 to 0.607.</br>\n",
    "The mcc score improved from 0.359 to 0.421"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (Standardization) (Patrick)\n",
    "\n",
    "Normalization (standardization) is a type of feature scaling that adjusts the values of your features to a standard distribution, such as a normal (or Gaussian) distribution, or a uniform distribution. This helps to reduce the skewness, outliers, or heteroscedasticity of your data, which can affect the performance or accuracy of your predictive models. By normalizing the data, it can be ensured that each feature contributes equally to the model and that the model is not biased towards any particular feature.\n",
    "\n",
    "Four common normalization techniques are scaling to a range, clipping, log scaling, and z-score.\n",
    "\n",
    "Source: https://developers.google.com/machine-learning/data-prep/transform/normalization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality - Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (Patrick)\n",
    "Feature selection is the process of selecting a subset of relevant features from the dataset that can help improve the accuracy, performance, or interpretability of your predictive models. By reducing the number of features, it can reduce the complexity of the model, avoid overfitting, and speed up training and inference. Having irrelevant features in the data can actually decrease the accuracy of the machine learning models.\n",
    "\n",
    "The top reasons to use feature selection are:\n",
    "* It enables the machine learning algorithm to train faster.\n",
    "* It reduces the complexity of a model and makes it easier to interpret.\n",
    "* It improves the accuracy of a model if the right subset is chosen.\n",
    "* It reduces overfitting.\n",
    "\n",
    "Source: https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "Dimensionality reduction is another technique used in feature engineering that can help reduce the number of features in your dataset while preserving the most important information or patterns. This can help improve the performance, accuracy, or interpretability of your predictive models, especially when dealing with high-dimensional data or noisy data. Some common techniques for dimensionality reduction include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), t-SNE, and Autoencoders ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Combination (Marco)\n",
    "Feature combination is another technique used in feature engineering that can help  create new features by combining or interacting existing features in your dataset. For example, by creating a new feature by multiplying two existing features, or by adding or subtracting two existing features. This can help capture more complex relationships or interactions between features and improve the performance or accuracy of predictive models.\n",
    "\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preparing data\n",
      "running pipeline\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1 1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m remove \u001b[39m=\u001b[39m RemoveFeatureTransformer([\u001b[39m'\u001b[39m\u001b[39mgeo_level_1_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgeo_level_2_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgeo_level_3_id\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m testpip \u001b[39m=\u001b[39m CustomPipeline([     \n\u001b[0;32m      7\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mcombine_geoLevels\u001b[39m\u001b[39m'\u001b[39m, combine),\n\u001b[0;32m      8\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mremove_singleGeoLevels\u001b[39m\u001b[39m'\u001b[39m, remove),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         skip_evaluation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m         skip_storing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m testpip\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\users\\marco\\workspace\\phase-1\\src\\pipelines\\build_pipelines.py:163\u001b[0m, in \u001b[0;36mCustomPipeline.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39m# ---------- Start the pipeline -------------\u001b[39;00m\n\u001b[0;32m    162\u001b[0m pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline \u001b[39m#Pipeline(steps=self.steps)\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_train, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_train)\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_evaluation):\n\u001b[0;32m    167\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mevaluating pipeline\u001b[39m\u001b[39m'\u001b[39m)        \n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:186\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    184\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 186\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    187\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[0;32m    188\u001b[0m )\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    190\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:579\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[0;32m    578\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 579\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_X_params)\n\u001b[0;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[0;32m    581\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:1998\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1997\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1998\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(values, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m   1999\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2000\u001b[0m         astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2001\u001b[0m         \u001b[39mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2002\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mis_single_block\n\u001b[0;32m   2003\u001b[0m     ):\n\u001b[0;32m   2004\u001b[0m         \u001b[39m# Check if both conversions can be done without a copy\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m         \u001b[39mif\u001b[39;00m astype_is_view(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m astype_is_view(\n\u001b[0;32m   2006\u001b[0m             values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype\n\u001b[0;32m   2007\u001b[0m         ):\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1 1'"
     ]
    }
   ],
   "source": [
    "# combine geo_level_1_id and geo_level_2_id\n",
    "estimator = DecisionTreeClassifier()\n",
    "combine = CombineFeatureTransformer('geo_level_1_id', 'geo_level_2_id')\n",
    "remove = RemoveFeatureTransformer(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'])\n",
    "\n",
    "testpip = CustomPipeline([     \n",
    "        ('combine_geoLevels', combine),\n",
    "        ('remove_singleGeoLevels', remove),\n",
    "        ('estimator', estimator)\n",
    "        ],\n",
    "        skip_evaluation=False,\n",
    "        skip_storing=True)\n",
    "testpip.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombine - Process\n",
    "In our data there are two types of features that are (almost) one-hot-encoded. Particulary 'has_superstructure_X' and 'has_secondary_use_X'. It could be useful to reconstruct the original categorical features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Feature Engineering (Thomas)\n",
    "\n",
    "* Analysing the relation of the new features to the target value\n",
    "* Evaluate a simple prediction model with different feature sets\n",
    "* Analyse the importance of the new features in the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
