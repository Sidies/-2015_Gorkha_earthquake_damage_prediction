{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In the feature engineering step, we create new features or modify existing ones to improve the performance of the machine learning model.\n",
    "\n",
    "Helpful Links:\n",
    "https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic - Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Encoding is a process used to transform categorical data into numerical values that can be understood by machine learning algorithms. There are several types of encoding techniques used in feature engineering such as one-hot encoding and label encoding. Some of the most commonly used techniques are:\n",
    "\n",
    "* One-hot encoding\n",
    "* Label encoding\n",
    "* Binary encoding\n",
    "* Count encoding\n",
    "* Target encoding\n",
    "* Hashing encoding\n",
    "\n",
    "One-hot encoding is a technique used to convert categorical data into numerical data by creating a binary vector for each category. For example, if we have a categorical feature called “color” with three categories (red, green, and blue), we can create three binary vectors (one for each category) with a value of 1 for the corresponding category and 0 for the others.\n",
    "\n",
    "Label encoding is another technique used to convert categorical data into numerical data by assigning a unique integer value to each category. For example, if we have a categorical feature called “color” with three categories (red, green, and blue), we can assign the values 0, 1, and 2 to each category respectively.\n",
    "\n",
    "Binary encoding is similar to one-hot encoding but uses fewer features. Count encoding replaces each category with the number of times it appears in the dataset. Target encoding replaces each category with the mean target value for that category. Hashing encoding is a technique that maps each category to a fixed-length vector\n",
    "\n",
    "Source: https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization\n",
    "\n",
    "Discretization is a process used to transform continuous data into categorical data. It involves dividing the range of a continuous variable into a set of intervals or bins and then assigning each value to the corresponding bin.\n",
    "\n",
    "Binning or discretization is used for the transformation of a continuous or numerical variable into a categorical feature. Binning of continuous variable introduces non-linearity and tends to improve the performance of the model. It can also be used to identify missing values or outliers\n",
    "\n",
    "Discretization can help improve the classifier by reducing the noise in the data and making it easier for the classifier to identify patterns. By discretizing continuous variables, they may be transformed into categorical variables that are easier to work with. This can help improve the accuracy of the classifier by reducing the number of features and making it easier to identify which features are most important.\n",
    "\n",
    "The effectiveness of discretization can depend on the model applied. Some models may be more sensitive to the choice of discretization method than others. Many machine learning algorithms perform better when tey are trained with discrete variables. For example, decision trees and random forests can benefit from discretization because they work best with categorical variables. On the other hand, linear regression models may not benefit as much from discretization because they work best with continuous variables\n",
    "\n",
    "Source: https://towardsdatascience.com/an-intro-to-discretization-techniques-for-machine-learning-93dce1198e68"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (Standardization)\n",
    "\n",
    "Normalization (standardization) is a type of feature scaling that adjusts the values of your features to a standard distribution, such as a normal (or Gaussian) distribution, or a uniform distribution. This helps to reduce the skewness, outliers, or heteroscedasticity of your data, which can affect the performance or accuracy of your predictive models. By normalizing the data, it can be ensured that each feature contributes equally to the model and that the model is not biased towards any particular feature.\n",
    "\n",
    "Four common normalization techniques are scaling to a range, clipping, log scaling, and z-score.\n",
    "\n",
    "Source: https://developers.google.com/machine-learning/data-prep/transform/normalization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality - Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Feature selection is the process of selecting a subset of relevant features from the dataset that can help improve the accuracy, performance, or interpretability of your predictive models. By reducing the number of features, it can reduce the complexity of the model, avoid overfitting, and speed up training and inference. Having irrelevant features in the data can actually decrease the accuracy of the machine learning models.\n",
    "\n",
    "The top reasons to use feature selection are:\n",
    "* It enables the machine learning algorithm to train faster.\n",
    "* It reduces the complexity of a model and makes it easier to interpret.\n",
    "* It improves the accuracy of a model if the right subset is chosen.\n",
    "* It reduces overfitting.\n",
    "\n",
    "Source: https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "Dimensionality reduction is another technique used in feature engineering that can help reduce the number of features in your dataset while preserving the most important information or patterns. This can help improve the performance, accuracy, or interpretability of your predictive models, especially when dealing with high-dimensional data or noisy data. Some common techniques for dimensionality reduction include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), t-SNE, and Autoencoders ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Combination\n",
    "Feature combination is another technique used in feature engineering that can help  create new features by combining or interacting existing features in your dataset. For example, by creating a new feature by multiplying two existing features, or by adding or subtracting two existing features. This can help capture more complex relationships or interactions between features and improve the performance or accuracy of predictive models.\n",
    "\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombine - Process\n",
    "In our data there are two types of features that are (almost) one-hot-encoded. Particulary 'has_superstructure_X' and 'has_secondary_use_X'. It could be useful to reconstruct the original categorical features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Feature Engineering\n",
    "\n",
    "* Analysing the relation of the new features to the target value\n",
    "* Evaluate a simple prediction model with different feature sets\n",
    "* Analyse the importance of the new features in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables referencing modules in repository\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics as stat \n",
    "import pipeline as pipe\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from src.features import build_features\n",
    "# from src.data import make_dataset \n",
    "# commented out because: there seems to be an issue at the moment with the initial method from make_dataset\n",
    "from src.models import train_model\n",
    "from src.models import predict_model\n",
    "from src.visualization import visualize\n",
    "from tabulate import tabulate\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "preparing data\n",
      "running pipeline\n",
      "storing models and predictions\n"
     ]
    }
   ],
   "source": [
    "pipe.run(calledFromNotebook=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
